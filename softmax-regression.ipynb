{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with TensorFlow\n",
    "### Load data \n",
    "Load the MNIST data using helper functions in tensorFlow. It will download the data if not already present in your computer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contain images of digits and corresponding labels. They are already separated into training, testing and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimension =  (55000, 784) Labels dimension =  (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print \"Image dimension = \", mnist.train.images.shape, \"Labels dimension = \", mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a basic softmax Regression\n",
    "The MNIST images have 28x28 = 784 dimension, but the dataset has been flattened for purposes of training a softmax regression. This will destroy the spatial structure present in the image. We will exploit the spatial structure when we use convolutional nets. \n",
    "\n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "Very simply, we compute the evidence that any image $x$ belongs to class $i$ as \n",
    "\n",
    "\\begin{equation*}\n",
    "Evidence_i = \\sum_j{W_{i,j}x_j+b_i} \n",
    "\\end{equation*}\n",
    "where $W_{i,j}$ is the weight and $b_i$ are the biases which we will learn during the training phase. \n",
    "\n",
    "Next, we use the softmax as an activation or link function to convert our linear evidence to probability distribution over 10 classes. \n",
    "\n",
    "Softmax is a simple function which is \n",
    "\\begin{equation*}\n",
    "softmax(x)_i = \\frac{\\exp{x_i}}{\\sum_j{\\exp{x_j}}}\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built the model \n",
    "We build a symbolic model using inbuilt functions in tensorflow. For defining input to the model, we can use placeholder variables with specified dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "To train a model, we need to define a **Cost or Loss Function** which we minimize during training to learn the weights and biases. The cost function we will use here is the _Cross Entropy_ function. \n",
    "\n",
    "\\begin{equation*}\n",
    "H_{\\tilde{y}}(y) = -\\sum_i{\\tilde{y}_i\\log{y_i}}\n",
    "\\end{equation*}\n",
    "where $y$ is our predicted class probabilities, and $\\tilde{y}$ is the true class probability (the one-hot vector we'll input). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_steps = 2000\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9177\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, we got somewhere around 92% accuracy in classifying MNIST dataset with our simplistic model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Tensorflow approach \n",
    "In tensorflow, you first build up your computation model (e.g. feedfordward neural network), and then train the model inside a session. A lot of things happen under the hood, e.g. the back-propagation and weight updates.  \n",
    "\n",
    "Tensorflow simplifies experimenting with new models quickly, and the models can be trained on a CPU or a GPU without writing any extra line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
